---
- name: Set up Local LLM Host (vLLM + AnythingLLM + Tailscale)
  hosts: localhost
  become: true
  vars:
    # pick SUDO_USER when running via sudo, otherwise fallback to USER
    username: "{{ lookup('env','SUDO_USER') | default(lookup('env','USER')) }}"
    llm_root: "/home/{{ lookup('env','SUDO_USER') | default(lookup('env','USER')) }}/llm-stack"
    vllm_model: "Qwen/Qwen2.5-7B-Instruct"
    vllm_api_key: "token-local"    # WARNING: avoid committing real secrets; use vault or env
    anythingllm_port: 3001
    vllm_port: 8000
    tailscale_authkey: ""   # Set your Tailscale reusable auth key here

  tasks:
    - name: Update system
      apt:
        update_cache: yes
        upgrade: dist

    - name: Install prerequisites
      apt:
        name:
          - curl
          - ca-certificates
          - gnupg
          - lsb-release
          - apt-transport-https
          - software-properties-common
          - docker.io
          - docker-compose-plugin
        state: present

    - name: Add user to docker group
      user:
        name: "{{ username }}"
        groups: docker
        append: yes

    # --- NVIDIA container runtime ---
    - name: Add NVIDIA container toolkit repo (shell)
      shell: |
        set -e
        curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
          gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit.gpg
        curl -fsSL https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
          tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
      args:
        executable: /bin/bash
      changed_when: false

    - name: Install NVIDIA container runtime
      apt:
        update_cache: yes
        name: nvidia-container-toolkit
        state: present

    - name: Configure NVIDIA runtime
      command: nvidia-ctk runtime configure --runtime=docker
      register: nvidia_ctk_result
      changed_when: "'configured' in (nvidia_ctk_result.stdout | lower) or nvidia_ctk_result.rc == 0"
      failed_when: nvidia_ctk_result.rc != 0 and nvidia_ctk_result.rc is not none

    - name: Restart Docker
      service:
        name: docker
        state: restarted
        enabled: yes

    # --- Tailscale setup ---
    - name: Add Tailscale APT repository
      shell: |
        set -e
        curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/$(lsb_release -cs).noarmor.gpg | \
          tee /usr/share/keyrings/tailscale-archive-keyring.gpg &gt;/dev/null
        curl -fsSL https://pkgs.tailscale.com/stable/ubuntu/$(lsb_release -cs).tailscale-keyring.list | \
          tee /etc/apt/sources.list.d/tailscale.list &gt;/dev/null
      args:
        executable: /bin/bash
      changed_when: false

    - name: Install Tailscale
      apt:
        update_cache: yes
        name: tailscale
        state: present

    - name: Enable and start Tailscale service
      service:
        name: tailscaled
        state: started
        enabled: yes

    - name: Authenticate and bring Tailscale up (if auth key provided)
      shell: |
        tailscale up --authkey {{ tailscale_authkey }} --ssh --hostname local-llm-host
      args:
        executable: /bin/bash
      when: tailscale_authkey != ""

    # --- LLM stack setup ---
    - name: Create directories
      file:
        path: "{{ item }}"
        state: directory
        owner: "{{ username }}"
        group: "{{ username }}"
        mode: '0755'
      loop:
        - "{{ llm_root }}"
        - "{{ llm_root }}/models"
        - "{{ llm_root }}/anythingllm"
        - "{{ llm_root }}/anythingllm/data"

    - name: Create AnythingLLM env file
      copy:
        dest: "{{ llm_root }}/anythingllm/.env"
        owner: "{{ username }}"
        content: |
          STORAGE_DIR=/app/server/storage
          PORT={{ anythingllm_port }}

    - name: Create Docker Compose file
      copy:
        dest: "{{ llm_root }}/docker-compose.yml"
        owner: "{{ username }}"
        content: |
          version: "3.9"
          services:
            vllm:
              image: vllm/vllm-openai:latest
              command:
                - "vllm"
                - "serve"
                - "{{ vllm_model }}"
                - "--dtype"
                - "auto"
                - "--port"
                - "{{ vllm_port }}"
                - "--api-key"
                - "{{ vllm_api_key }}"
                - "--max-model-len"
                - "16000"
              environment:
                - NVIDIA_VISIBLE_DEVICES=all
              deploy: {}
              device_requests:
                - driver: nvidia
                  count: 1
                  capabilities: [gpu]
              ports:
                - "127.0.0.1:{{ vllm_port }}:{{ vllm_port }}"
              restart: unless-stopped
              volumes:
                - "{{ llm_root }}/models:/root/.cache/huggingface"

            anythingllm:
              image: mintplexlabs/anythingllm:latest
              depends_on:
                - vllm
              ports:
                - "127.0.0.1:{{ anythingllm_port }}:{{ anythingllm_port }}"
              cap_add:
                - SYS_ADMIN
              environment:
                - STORAGE_DIR=/app/server/storage
              volumes:
                - "{{ llm_root }}/anythingllm:/app/server/storage"
                - "{{ llm_root }}/anythingllm/.env:/app/server/.env"
              restart: unless-stopped

    - name: Create systemd service for LLM stack
      copy:
        dest: /etc/systemd/system/llm-stack.service
        content: |
          [Unit]
          Description=Local LLM Stack (vLLM + AnythingLLM)
          After=docker.service
          Requires=docker.service

          [Service]
          Type=oneshot
          WorkingDirectory={{ llm_root }}
          Environment=HOME=/home/{{ username }}
          ExecStart=/usr/bin/docker compose up -d
          ExecStop=/usr/bin/docker compose down
          RemainAfterExit=yes
          User={{ username }}
          Group={{ username }}

          [Install]
          WantedBy=multi-user.target
      notify: daemon-reload

    - name: Enable and start LLM stack
      systemd:
        name: llm-stack.service
        enabled: yes
        state: started

    # --- Optional RDP GUI ---
    - name: Install xrdp and XFCE
      apt:
        name:
          - xrdp
          - xfce4
          - xfce4-goodies
        state: present

    - name: Configure XRDP session
      copy:
        dest: /home/{{ username }}/.xsession
        content: |
          startxfce4
        owner: "{{ username }}"
        mode: '0644'

    - name: Add xrdp to ssl-cert group
      user:
        name: xrdp
        groups: ssl-cert
        append: yes

    - name: Enable xrdp service
      service:
        name: xrdp
        state: started
        enabled: yes

    # --- Shell aliases for convenience (idempotent block) ---
    - name: Add LLM management aliases to .bashrc
      blockinfile:
        path: "/home/{{ username }}/.bashrc"
        create: yes
        owner: "{{ username }}"
        mode: '0644'
        block: |
          # Local LLM aliases
          alias llm-up='cd ~/llm-stack &amp;&amp; docker compose up -d'
          alias llm-down='cd ~/llm-stack &amp;&amp; docker compose down'
          alias llm-logs='cd ~/llm-stack &amp;&amp; docker compose logs -f'

    # --- Firewall rules for LAN + Tailscale only ---
    - name: Warn about UFW reset (do not run automatically)
      debug:
        msg: |
          The playbook contains a UFW reset step. Running "ufw --force reset" may drop existing rules and
          could lock you out if you're connected remotely. Consider enabling SSH first or editing this task.
    # If you do want to run it automatically, uncomment / modify the following task and ensure SSH is allowed:
    # - name: Configure UFW
    #   shell: |
    #     ufw --force reset
    #     ufw default deny incoming
    #     ufw default allow outgoing
    #     ufw allow ssh
    #     ufw allow in on tailscale0
    #     ufw allow from 192.168.1.0/24
    #     ufw enable
    #   args:
    #     executable: /bin/bash

  handlers:
    - name: daemon-reload
      command: systemctl daemon-reload
